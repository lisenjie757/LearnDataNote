const e=JSON.parse('{"key":"v-51aee7f6","path":"/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/3.%20Transformer.html","title":"3.【经典网络】Transformer 论文精读","lang":"zh-CN","frontmatter":{"description":"3.【经典网络】Transformer 论文精读 原文链接：Attention Is All You Need 0. 核心总结 核心是提出了一种残差结构，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。","head":[["meta",{"property":"og:url","content":"https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/3.%20Transformer.html"}],["meta",{"property":"og:site_name","content":"Li Senjie"}],["meta",{"property":"og:title","content":"3.【经典网络】Transformer 论文精读"}],["meta",{"property":"og:description","content":"3.【经典网络】Transformer 论文精读 原文链接：Attention Is All You Need 0. 核心总结 核心是提出了一种残差结构，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-04-26T10:47:40.000Z"}],["meta",{"property":"article:author","content":"李森杰"}],["meta",{"property":"article:modified_time","content":"2023-04-26T10:47:40.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"3.【经典网络】Transformer 论文精读\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2023-04-26T10:47:40.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"李森杰\\",\\"url\\":\\"https://lisenjie757.github.io\\"}]}"]]},"headers":[{"level":2,"title":"0. 核心总结","slug":"_0-核心总结","link":"#_0-核心总结","children":[]},{"level":2,"title":"1. 摘要","slug":"_1-摘要","link":"#_1-摘要","children":[]},{"level":2,"title":"2. 引言","slug":"_2-引言","link":"#_2-引言","children":[]},{"level":2,"title":"3. ResNet网络细节","slug":"_3-resnet网络细节","link":"#_3-resnet网络细节","children":[{"level":3,"title":"3.1 如何增加残差连接","slug":"_3-1-如何增加残差连接","link":"#_3-1-如何增加残差连接","children":[]},{"level":3,"title":"3.2 网络架构","slug":"_3-2-网络架构","link":"#_3-2-网络架构","children":[]},{"level":3,"title":"3.3 网络训练细节","slug":"_3-3-网络训练细节","link":"#_3-3-网络训练细节","children":[]}]},{"level":2,"title":"4. 实验与结果","slug":"_4-实验与结果","link":"#_4-实验与结果","children":[]},{"level":2,"title":"5. 个人思考","slug":"_5-个人思考","link":"#_5-个人思考","children":[]}],"git":{"createdTime":1682506060000,"updatedTime":1682506060000,"contributors":[{"name":"lisenjie757","email":"1215954303@qq.com","commits":1}]},"readingTime":{"minutes":7.95,"words":2384},"filePathRelative":"知识库/文献精读/3. Transformer.md","localizedDate":"2023年4月26日","excerpt":"<h1> 3.【经典网络】Transformer 论文精读</h1>\\n<blockquote>\\n<p>原文链接：<a href=\\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Attention Is All You Need</a></p>\\n</blockquote>\\n<h2> 0. 核心总结</h2>\\n<p>核心是提出了一种<strong>残差结构</strong>，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。</p>","autoDesc":true}');export{e as data};
