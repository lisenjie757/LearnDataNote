<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="https://lisenjie757.github.io/rss.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://lisenjie757.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>Li Senjie</title>
    <link>https://lisenjie757.github.io/</link>
    <description>李森杰的个人主页</description>
    <language>zh-CN</language>
    <pubDate>Wed, 26 Apr 2023 14:27:05 GMT</pubDate>
    <lastBuildDate>Wed, 26 Apr 2023 14:27:05 GMT</lastBuildDate>
    <generator>vuepress-plugin-feed2</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <item>
      <title>lesson7. 保存和加载模型</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson7_saveload.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson7_saveload.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson7. 保存和加载模型</source>
      <description>lesson7. 保存和加载模型 import torch import torchvision.models as models</description>
      <pubDate>Wed, 26 Apr 2023 12:18:25 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson7. 保存和加载模型</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2> 保存和加载模型权重</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 保存和加载整个模型结构和权重</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2> 保存和加载训练时的断点</h2>
<h3> 1.导入必要的库</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3> 2.定义和初始化神经网络</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h3> 3.初始化优化器</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3> 4.保存一般断点</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3> 5.加载一般断点</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h3> 6.迁移学习下的热启动模式</h3>
<ul>
<li>只需设置strict=False来忽略非匹配的模型层参数</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson0. GPU性能测试</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/gpu_test.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/gpu_test.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson0. GPU性能测试</source>
      <description>lesson0. GPU性能测试 import torch import time print(torch.__version__) # 返回pytorch的版本 print(torch.cuda.is_available()) # 当CUDA可用时返回True a = torch.randn(10000, 1000) # 返回10000行1000列的张量矩阵 b = torch.randn(1000, 2000) # 返回1000行2000列的张量矩阵 t0 = time.time() # 记录时间 c = torch.matmul(a, b) # 矩阵乘法运算 t1 = time.time() # 记录时间 print(a.device, t1 - t0, c.norm(2)) # c.norm(2)表示矩阵c的二范数 device = torch.device(&amp;apos;cuda&amp;apos;) # 用GPU来运行 a = a.to(device) b = b.to(device) # 初次调用GPU，需要数据传送，因此比较慢 t0 = time.time() c = torch.matmul(a, b) t2 = time.time() print(a.device, t2 - t0, c.norm(2)) # 这才是GPU处理数据的真实运行时间，当数据量越大，GPU的优势越明显 t0 = time.time() c = torch.matmul(a, b) t2 = time.time() print(a.device, t2 - t0, c.norm(2))</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson0. GPU性能测试</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson1. 张量操作</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson1_tensor.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson1_tensor.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson1. 张量操作</source>
      <description>lesson1. 张量操作 import torch import numpy as np</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson1. 张量操作</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2> 初始化张量</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 张量的属性</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 张量运算</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 与Numpy互相转换</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson2. 数据集和数据加载</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson2_data.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson2_data.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson2. 数据集和数据加载</source>
      <description>lesson2. 数据集和数据加载 import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson2. 数据集和数据加载</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 加载预加载数据集</h2>
<ul>
<li>root ：训练集/测试集存储路径</li>
<li>train ：确定是训练集还是测试集</li>
<li>download=True ：如果root不可用 从互联网上下载数据</li>
<li>transform / target_transform ：指定特征和标签的转换</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 迭代和可视化数据集</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://i.imgur.com/Ui7pdoU.png" alt="result" tabindex="0" loading="lazy"><figcaption>result</figcaption></figure>
<h2> 创建自定义数据集</h2>
<ul>
<li>__init__ ：创建类对象时运行一次 包括图像 标注 和 变换</li>
<li>__len__ ：返回数据集样本数</li>
<li>__getitem__ ：根据索引加载和返回数据样本 包括图像 标注 和 变换</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 使用DataLoaders训练Data</h2>
<ul>
<li>DataLoader的作用：minibatches采样，每epoch reshuffle data，多线程加速</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 通过DataLoader迭代</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<figure><img src="https://i.imgur.com/UEoa2W0.png" alt="result" tabindex="0" loading="lazy"><figcaption>result</figcaption></figure>

]]></content:encoded>
      <enclosure url="https://i.imgur.com/Ui7pdoU.png" type="image/png"/>
    </item>
    <item>
      <title>lesson3. 数据预处理</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson3_transform.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson3_transform.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson3. 数据预处理</source>
      <description>lesson3. 数据预处理 图像和标签的转换函数 ToTensor() ：转换PIL图像和NumPy数组为FloatTensor，并归一化图像像素值为[0.,1.] Lamda Transforms ：此函数为用户自定义lamda函数；此处函数为将整数转换为one-hot编码，首先创建zero张量，长度取决于类别数，然后调用scatter_函数通过整数y对张量进行1填充 scatter_函数填充原理(dim=0) ：self[ index[i][j] ] [j] = src[i][j]</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson3. 数据预处理</h1>
<h2> 图像和标签的转换函数</h2>
<ul>
<li>ToTensor() ：转换PIL图像和NumPy数组为FloatTensor，并归一化图像像素值为[0.,1.]</li>
<li>Lamda Transforms ：此函数为用户自定义lamda函数；此处函数为将整数转换为one-hot编码，首先创建zero张量，长度取决于类别数，然后调用scatter_函数通过整数y对张量进行1填充</li>
<li>scatter_函数填充原理(dim=0) ：self[ index[i][j] ] [j] = src[i][j]</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>]]></content:encoded>
    </item>
    <item>
      <title>lesson4. 构建神经网络</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson4_build.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson4_build.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson4. 构建神经网络</source>
      <description>lesson4. 构建神经网络 import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets, transforms</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson4. 构建神经网络</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 获取训练设备</h2>
<ul>
<li>查看有无可用GPU</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 定义神经网络类</h2>
<ul>
<li>super(NeuralNetwork,self) ：查找NeuralNetwork的父类，对self实施父类的方法</li>
<li>nn.Flatten(x,[start=1,end=-1]) ：对输入张量进行指定维数降维，此处将(1,28,28)降成(1,28*28)</li>
<li>nn.Sequential() ：序列容器，将神经网络模块按顺序添加到容器中</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 模型层解构</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> nn.Flatten</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> nn.Linear</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> nn.ReLu</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> nn.Sequential</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> nn.Softmax</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 模型参数</h2>
<ul>
<li>nn.Moudle会自动跟踪保存模型参数，使用parameters()或named_parameters()获取</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson5. 自动求导机制</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson5_autograd.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson5_autograd.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson5. 自动求导机制</source>
      <description>lesson5. 自动求导机制 import torch 定义最简单的单层神经网络 ## input tensor x = torch.ones(5) ## expected output y = torch.zeros(3) w = torch.randn(5,3,requires_grad=True) b = torch.randn(3,requires_grad=True) z = torch.matmul(x,w) + b loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson5. 自动求导机制</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2> 定义最简单的单层神经网络</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 反向传播函数的引用存储在张量的grad_fn属性中</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 计算梯度</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 禁用梯度跟踪</h2>
<ul>
<li>有时已有训练好的模型，我们只想推理，不想更新参数，可以使用torch.no_grad()或detach()来禁用梯度计算</li>
<li>以下是可能要禁用梯度跟踪的几点理由：1.做微调时可能需要冻结某些参数 2.只做推理时提高计算效率</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 张量梯度和Jacobian Products</h2>
<ol>
<li>PyTorch backward grad实际计算保存的是Jacobian Products=vT*J，默认v=tensor(1.0)</li>
<li>PyTorch的机制会累积梯度，所以需要清零，实际训练时优化器会自动帮助我们清零</li>
</ol>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson6. 优化机制</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson6_optimization.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson6_optimization.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson6. 优化机制</source>
      <description>lesson6. 优化机制 import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor,Lambda</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson6. 优化机制</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 前置代码</h2>
<h3> 数据集&amp;数据加载器&amp;构建模型</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3> 超参数</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 优化循环</h2>
<h3> 损失函数</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3> 优化器</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2> 完整实现</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>0.【方法论】如何阅读论文</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/0.%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95%E8%AE%BA.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/0.%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95%E8%AE%BA.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">0.【方法论】如何阅读论文</source>
      <description>0.【方法论】如何阅读论文 1 【吴恩达】读论文的方法 吴恩达cs230课程中的一节，视频链接：link 1.1 收集论文</description>
      <pubDate>Wed, 26 Apr 2023 10:47:40 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 0.【方法论】如何阅读论文</h1>
<h2> 1 【吴恩达】读论文的方法</h2>
<p>吴恩达cs230课程中的一节，视频链接：<a href="https://www.bilibili.com/video/BV195411Y7ms/?from=search&amp;seid=6380620962674373232&amp;spm_id_from=333.337.0.0&amp;vd_source=e8390493db96c0c1ca516a431544c5d8" target="_blank" rel="noopener noreferrer">link</a></p>
<h3> 1.1 收集论文</h3>
<ol>
<li>编制论文清单（arxiv，github，blog）</li>
<li>快速浏览，从清单中进行挑选</li>
<li>决定你要关注哪些论文</li>
</ol>
<blockquote>
<p>5-20篇论文：对某一领域有基本的了解，可以应用一些算法<br>
50-100篇论文：对某一领域有了很好的了解</p>
</blockquote>
<h3> 1.2 多遍读论文</h3>
<ol>
<li>标题+摘要+图表</li>
<li>引言+结论+图表+略读剩余内容</li>
<li>通读整篇论文但跳过详细的数学公式和相关工作</li>
<li>精读整篇论文但跳过没有意义的部分</li>
</ol>
<h3> 1.3 读完后测试自己或与其他人讨论这些问题</h3>
<ol>
<li>作者试图实现什么？</li>
<li>核心要点是什么？</li>
<li>我能从中使用什么？</li>
<li>有没有其他想要关注的参考文献？</li>
</ol>
<h3> 1.4 去哪里跟随ML新前沿</h3>
<ol>
<li>Twitter（Kian, Andrew NG）</li>
<li>ML Subreddit</li>
<li>NIPS/ICML/ICLR</li>
<li>Friends</li>
<li>Arxiv Sanity</li>
</ol>
<h3> 1.5 如何深入理解数学部分</h3>
<ol>
<li>自己在白纸上从头开始推导一遍</li>
</ol>
<h3> 1.6 如何深度理解算法（代码）部分</h3>
<ol>
<li>从开源仓库（github）下载并跑通</li>
<li>学习开源代码，自己重新从头实现一遍</li>
</ol>
<h3> 1.7 长期持续学习的建议</h3>
<ol>
<li>稳步持续地阅读，而不是短期突击，例如<strong>每周稳定阅读2～3篇文献</strong></li>
</ol>
<hr>
<h2> 2 【吴恩达】职业生涯规划的建议</h2>
<h3> 2.1 找工作（面试官）看重什么</h3>
<ol>
<li>必备技能：面试题和代码能力
<ul>
<li>面试题例子：梯度下降/批梯度下降/随机梯度下降是什么？平均批大小太大或太小会发生什么？</li>
</ul>
</li>
<li>有意义的工作（证明你有应用能力）</li>
<li>持续学习的能力</li>
<li>知识的广度和深度
<ul>
<li>广度：对一个领域有横向的了解，例如AI的众多分支ML，DL，PGM，NLP，CV</li>
<li>深度：对某一子方向进行了深入的工作，例如项目经历，科研经历，<strong>实习经历</strong>，参与建设开源代码</li>
</ul>
</li>
</ol>
<h3> 2.2 在校时的失败模式</h3>
<ol>
<li>一直上课是不好的：通过上课（课程作业，论文）掌握某一领域的基础知识和基础技能后，更深入、更有针对性地参与1～2个感兴趣的项目</li>
<li>一进来就在参与某一领域很深入也是不好的：还没有学到很多关于编程的知识或机器学习并立即进入研究项目，这对学生来说并不有效，甚至更糟</li>
<li>有很大的广度，做了很多个小项目也是不好的：有10个项目但没有1～2个真正重要的项目，<strong>面试官对数量不感兴趣</strong></li>
<li><strong>在周末卷是没有用的，关键是做好每周的规划</strong>：关键是每周都能抽出时间持续地执行自己的学习计划（阅读论文或者贡献开源或参加一些课程），这样才能有稳定、持续的进步，例如每周阅读2～3篇论文，一年就至少有了100篇论文的积累，而不是光想着花周末的时间去卷 👍👍👍</li>
</ol>
<h3> 2.3 如何选择好的工作</h3>
<ol>
<li>选择一个有很棒的人和项目的团队：因为我们都会深深受身边人的影响，他们能教你多少，他们的努力程度，他们的眼界格局、人脉资源等都会深刻地影响我们未来的发展</li>
<li><strong>选择一个好的小团队，特别是好的leader，而不是公司的品牌</strong>：工作时接触最多的是团队中的10～30个人，特别是直接带你的leader，这才是选职位时应该着重关注的，而不是公司品牌大小，大公司内部方差极大，所以宁愿去小公司的精英团队，也不要去大公司的垃圾团队 👍👍👍</li>
<li>如果在找工作时公司方拒绝告诉你做什么项目，leader是谁，那就该好好掂量掂量这份工作的含金量了</li>
<li>注意公司的轮换制度：千万要小心进入公司后等公司分配的工作，最好事先就确定好团队，如果公司提前保证了将来会让你轮换到你心仪的团队，那还是可以接受的</li>
<li>多学习，多做重要的工作！！！</li>
</ol>
<h3> 2.4 【学生提问】什么是有意义的工作</h3>
<ol>
<li>大公司的AI偏同质化，都在卷最赚钱的几个赛道，例如人脸识别，语音识别，搜广推等等</li>
<li>但其实AI可以和许多传统行业相结合，改造传统行业</li>
<li>挖掘冷门的新赛道也是一件激动人心，具有挑战性的事情，也许会从中发现更好的机会</li>
</ol>
<hr>
<h2> 3 【李沐】如何读论文</h2>
<p>视频连接：<a href="https://www.bilibili.com/video/BV1H44y1t75x/?spm_id_from=333.999.0.0&amp;vd_source=e8390493db96c0c1ca516a431544c5d8" target="_blank" rel="noopener noreferrer">link</a></p>
<h3> 3.1 论文结构</h3>
<ol>
<li>titile</li>
<li>abstract</li>
<li>introduction</li>
<li>method</li>
<li>experiments</li>
<li>conclusion</li>
</ol>
<h2> 3.2 三步法读文献</h2>
<ul>
<li>第一遍：标题、摘要、结论，看看是否和自己研究的相关。看完之后可以看看方法和实验部分重要的图和表。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。</li>
<li>第二遍：确定论文值得读之后，忽略部分细节，快速的把整个论文过一遍，需要了解重要的图和表，看懂文章针对的问题、解决方法、效果怎么样，知道每一个部分在干什么，圈出相关文献。觉得文章有看不懂的地方，可以读相关的引用文献。</li>
<li>第三遍：最详细的一遍，知道每一段和每一句干什么，脑补过程、换位思考、进行延伸</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>1.【经典网络】AlexNet 论文精读</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/1.%20AlexNet.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/1.%20AlexNet.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">1.【经典网络】AlexNet 论文精读</source>
      <description>1.【经典网络】AlexNet 论文精读 原文链接：ImageNet Classification with Deep Convolutional Neural Networks 0. 核心总结 核心是提出了一种深层卷积神经网络的架构，并引入了GPU训练、ReLU激活函数、Dropout正则化这些训练深层卷积神经网络的手段。</description>
      <pubDate>Wed, 26 Apr 2023 10:47:40 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 1.【经典网络】AlexNet 论文精读</h1>
<blockquote>
<p>原文链接：<a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank" rel="noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks</a></p>
</blockquote>
<h2> 0. 核心总结</h2>
<p>核心是提出了一种深层卷积神经网络的架构，并引入了GPU训练、ReLU激活函数、Dropout正则化这些训练深层卷积神经网络的手段。</p>
<h2> 1. 摘要</h2>
<p>训练了一个大型的卷积神经网络，在ImageNet LSVRC-2010和ILSVRC-2012图像分类竞赛中取得了远低于第二名错误率的成绩。</p>
<h2> 2. 引言</h2>
<p>事实证明，大图像数据集比小图像数据集能够训练更优的模型，但同时对于ImageNet这样的超大型数据集也需要与之对应的具有强大学习能力的模型，CNN对图像的性质（局部平稳和参数共享）使其比相同性能的标准前馈神经网络参数量要少得多。</p>
<p>尽管CNN具有如此优秀的性质，但计算它还是很昂贵的，然而得益于GPU使得训练大型的CNN网络成为了可能，并且大型图像数据集ImageNet的出现使得模型不会出现严重的过拟合。</p>
<p>本文的具体贡献如下：</p>
<ol>
<li>在ImageNet子集上训练了迄今为止最大的卷积神经网络，并取得了迄今为止最好的成绩；</li>
<li>编写了一个高度优化的2D卷积的GPU实现，以及提供了训练卷积神经网络的所有操作细节，包括如何提高性能、减少训练时间等；</li>
<li>即使有大量的数据，但由于网络的太大还是会有过拟合的问题，所以采用了几种有效的技术来防止过拟合。</li>
</ol>
<h2> 3. 数据集</h2>
<p>使用了著名的ImageNet数据集，介绍直接从原文翻译。</p>
<blockquote>
<p>ImageNet是一个由超过1500万张标注的高分辨率图像组成的数据集，属于大约22，000个类别。这些图像是从网上收集的，并由人类贴标者使用亚马逊的研究工具众包工具进行标记。从2010年开始，作为Pascal视觉对象挑战赛的一部分，每年举办一次名为ImageNet大规模视觉识别挑战赛( ILSVRC )的竞赛。ILSVRC使用ImageNet的一个子集，在1000个类别中的每个类别中大约有1000个图像。总共有大约120万张训练图像、50,000张验证图像和150,000张测试图像。<br>
ILSVRC-2010是唯一一个测试集标签可用的ILSVRC版本，因此这是我们进行大部分实验的版本。由于我们也在ILSVRC-2012竞赛中输入了我们的模型，在第6节中我们也报告了我们在这个版本的数据集上的结果，对于这个版本的数据集，测试集标签是不可用的。在ImageNet上，通常报告两个错误率：top-1和top-5，其中top-5错误率是模型认为最可能的5个标签中没有正确标签的测试图像的分数。<br>
ImageNet由可变分辨率的图像组成，而我们的系统需要一个恒定的输入维度。因此，我们将图像降采样到256×256的固定分辨率。给定一个矩形图像，我们首先对图像进行缩放，使较短的边长为256，然后从结果图像中裁剪出中心的256×256块。我们没有以任何其他方式预处理图像，除了从每个像素中减去训练集的平均活动。因此，我们在像素的(中心)原始RGB值上训练我们的网络。</p>
</blockquote>
<h2> 4. 架构</h2>
<p>AlexNet的网络架构如下图所示，它包含5个卷积层和3个全连接层，下面会介绍AlexNet网络一些新颖的特征，以下特征根据重要性向下排序。</p>
<figure><img src="https://i.imgur.com/5F8HDJg.png" alt="AlexNet网络架构图" tabindex="0" loading="lazy"><figcaption>AlexNet网络架构图</figcaption></figure>
<h3> 4.1 ReLU激活函数</h3>
<p>ReLU激活函数的公式：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p>
<p>本文通过大量实验证明使用ReLU作为激活函数比使用tanh作激活函数训练速度要快好几倍，如下图所示：</p>
<figure><img src="https://i.imgur.com/uotVyNs.png" alt="relu与tanh速度对比图" tabindex="0" loading="lazy"><figcaption>relu与tanh速度对比图</figcaption></figure>
<h3> 4.2 多GPU训练</h3>
<p>本文使用两块GTX 580 GPU来训练AlexNet，采用并行化方案，每个GPU上放置一半的内核。</p>
<h3> 4.3 局部响应归一化（LRN，Local Response Normalization）</h3>
<p>一种提高泛化性的手段，现在已不常用，被批归一化Batch Normalization代替。</p>
<h3> 4.4 重叠池化（Overlapping Pooling）</h3>
<p>即步长要小于池化单元的大小，使得每一个池化单元组成的网格有所重叠，这样能稍微降低点错误率。</p>
<h3> 4.5 整体架构</h3>
<p>AlexNet包含8个带权重的层，前5个为卷积层，其余3个为全连接层，最后输出到一个1000类的Softmax层。</p>
<p>第2层，第4层，第5层卷积层内核仅连接到位于同一GPU上的前一层内核。第3层内核连接到第二层的所有内核，全连接层也与上一层的所有神经元相连。局部相应归一化应用于第1层和第2层，最大池化层是重叠池化，使用在第1层，第2层和第5层。</p>
<h2> 5. 减少过拟合</h2>
<p>第一种方式是使用数据增强，包括以下两种形式：</p>
<ol>
<li>通过图像平移和水平反射生成图像；</li>
<li>改变训练图像中RGB通道的强度，即对整个ImageNet训练集的RGB像素值集合进行PCA主成分分析，将权重加到对应的RGB通道上。</li>
</ol>
<p>第二种方式是使用Dropout方法，即在每轮前向传播和反向传播中，每层的神经元将会有一定的比例“失活”，即在本轮置为0，不参与前向传播和反向传播的过程。</p>
<h2> 6. 训练细节</h2>
<p>优化器为SGD，batch size为128，momentum为0.9，权重衰减（L2正则）为5e-4。</p>
<p>从一个均值为0，标准差为0.01的高斯分布中初始化各层权重，用常数1初始化第2层，第4层，第5层和全连接层的偏置，用常数0初始化其余层的偏置。</p>
<p>每层的学习率相同，并在训练过程中手动调整。学习率初始化为0.01，当验证集错误率停止下降时，将学习率减小10倍继续训练，在终止训练前减小3次。</p>
<p>本文在120万张图像训练集上训练了90个epoch，在两个NVDIA GTX 580 3GB GPU上训练了5～6天的时间。</p>
<h2> 7. 结果</h2>
<p>在ILSVRC-2010测试集上与其它方法的对比结果：</p>
<figure><img src="https://i.imgur.com/9ZvQf02.png" alt="AlexNet测试结果图1" tabindex="0" loading="lazy"><figcaption>AlexNet测试结果图1</figcaption></figure>
<p>在ILSVRC-2012验证集和测试集上的对比结果，*号表示在完整版ImageNet数据集上进行过预训练再微调，1 CNN表示一个标准的AlexNet，5 CNN表示训练5个AlexNet将预测结果取平均，7 CNN表示在AlexNet最后一个池化层后再加上第6个卷积层：</p>
<figure><img src="https://i.imgur.com/A8guPdw.png" alt="AlexNet测试结果图2" tabindex="0" loading="lazy"><figcaption>AlexNet测试结果图2</figcaption></figure>
<p>下图展示了第1层卷积层的96个卷积核的可视化情况，表征了卷积核所学习到的特征：</p>
<figure><img src="https://i.imgur.com/f3WtVFM.png" alt="AlexNet卷积核可视化" tabindex="0" loading="lazy"><figcaption>AlexNet卷积核可视化</figcaption></figure>
<p>下图展示了AlexNet在8张测试图像上的top-5预测结果，可以发现即使是偏离中心的对象（如第1幅图）也可以被网络识别，并且大多数图像top-5预测结果尽管并非完全正确但似乎也是合理的，同时也可以注意到部分图像（如第7幅图）本身就具有模糊性：</p>
<figure><img src="https://i.imgur.com/2OnlTqo.png" alt="AlexNet图像预测" tabindex="0" loading="lazy"><figcaption>AlexNet图像预测</figcaption></figure>
<p>下图展示了在AlexNet最后一个隐藏层的特征向量欧氏距离最小的几幅图像（每行），可以发现特征向量欧式距离相近的图片确实表征的是同一个对象，即使对象的位置、朝向、明暗不同，这表明卷积神经网络可以很好的提取出图像的语义特征</p>
<figure><img src="https://i.imgur.com/9ivEjgY.jpg" alt="AlexNet最后一个隐藏层所表征的特征向量" tabindex="0" loading="lazy"><figcaption>AlexNet最后一个隐藏层所表征的特征向量</figcaption></figure>
<h2> 8. 思考</h2>
<p>本文认为卷积神经网络的深度是很重要的，减小深度会导致性能的下降。</p>
<p>AlexNet虽然在图像分类任务上取得了很大的提升，但对比人类视觉系统的识别还是有一定差距的。</p>
]]></content:encoded>
      <enclosure url="https://i.imgur.com/5F8HDJg.png" type="image/png"/>
    </item>
  </channel>
</rss>