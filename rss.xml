<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="https://lisenjie757.github.io/rss.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://lisenjie757.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>Li Senjie</title>
    <link>https://lisenjie757.github.io/</link>
    <description>李森杰的个人主页</description>
    <language>zh-CN</language>
    <pubDate>Thu, 04 May 2023 02:22:59 GMT</pubDate>
    <lastBuildDate>Thu, 04 May 2023 02:22:59 GMT</lastBuildDate>
    <generator>vuepress-plugin-feed2</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <item>
      <title>0.【方法论】如何阅读论文</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/0.%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95%E8%AE%BA.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/0.%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95%E8%AE%BA.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">0.【方法论】如何阅读论文</source>
      <description>0.【方法论】如何阅读论文 1 【吴恩达】读论文的方法 吴恩达cs230课程中的一节，视频链接：link 1.1 收集论文</description>
      <pubDate>Wed, 03 May 2023 05:18:33 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 0.【方法论】如何阅读论文</h1>
<h2> 1 【吴恩达】读论文的方法</h2>
<p>吴恩达cs230课程中的一节，视频链接：<a href="https://www.bilibili.com/video/BV195411Y7ms/?from=search&amp;seid=6380620962674373232&amp;spm_id_from=333.337.0.0&amp;vd_source=e8390493db96c0c1ca516a431544c5d8" target="_blank" rel="noopener noreferrer">link</a></p>
<h3> 1.1 收集论文</h3>
<ol>
<li>编制论文清单（arxiv，github，blog）</li>
<li>快速浏览，从清单中进行挑选</li>
<li>决定你要关注哪些论文</li>
</ol>
<blockquote>
<p>5-20篇论文：对某一领域有基本的了解，可以应用一些算法<br>
50-100篇论文：对某一领域有了很好的了解</p>
</blockquote>
<h3> 1.2 多遍读论文</h3>
<ol>
<li>标题+摘要+图表</li>
<li>引言+结论+图表+略读剩余内容</li>
<li>通读整篇论文但跳过详细的数学公式和相关工作</li>
<li>精读整篇论文但跳过没有意义的部分</li>
</ol>
<h3> 1.3 读完后测试自己或与其他人讨论这些问题</h3>
<ol>
<li>作者试图实现什么？</li>
<li>核心要点是什么？</li>
<li>我能从中使用什么？</li>
<li>有没有其他想要关注的参考文献？</li>
</ol>
<h3> 1.4 去哪里跟随ML新前沿</h3>
<ol>
<li>Twitter（Kian, Andrew NG）</li>
<li>ML Subreddit</li>
<li>NIPS/ICML/ICLR</li>
<li>Friends</li>
<li>Arxiv Sanity</li>
</ol>
<h3> 1.5 如何深入理解数学部分</h3>
<ol>
<li>自己在白纸上从头开始推导一遍</li>
</ol>
<h3> 1.6 如何深度理解算法（代码）部分</h3>
<ol>
<li>从开源仓库（github）下载并跑通</li>
<li>学习开源代码，自己重新从头实现一遍</li>
</ol>
<h3> 1.7 长期持续学习的建议</h3>
<ol>
<li>稳步持续地阅读，而不是短期突击，例如<strong>每周稳定阅读2～3篇文献</strong></li>
</ol>
<hr>
<h2> 2 【吴恩达】职业生涯规划的建议</h2>
<h3> 2.1 找工作（面试官）看重什么</h3>
<ol>
<li>必备技能：面试题和代码能力
<ul>
<li>面试题例子：梯度下降/批梯度下降/随机梯度下降是什么？平均批大小太大或太小会发生什么？</li>
</ul>
</li>
<li>有意义的工作（证明你有应用能力）</li>
<li>持续学习的能力</li>
<li>知识的广度和深度
<ul>
<li>广度：对一个领域有横向的了解，例如AI的众多分支ML，DL，PGM，NLP，CV</li>
<li>深度：对某一子方向进行了深入的工作，例如项目经历，科研经历，<strong>实习经历</strong>，参与建设开源代码</li>
</ul>
</li>
</ol>
<h3> 2.2 在校时的失败模式</h3>
<ol>
<li>一直上课是不好的：通过上课（课程作业，论文）掌握某一领域的基础知识和基础技能后，更深入、更有针对性地参与1～2个感兴趣的项目</li>
<li>一进来就在参与某一领域很深入也是不好的：还没有学到很多关于编程的知识或机器学习并立即进入研究项目，这对学生来说并不有效，甚至更糟</li>
<li>有很大的广度，做了很多个小项目也是不好的：有10个项目但没有1～2个真正重要的项目，<strong>面试官对数量不感兴趣</strong></li>
<li><strong>在周末卷是没有用的，关键是做好每周的规划</strong>：关键是每周都能抽出时间持续地执行自己的学习计划（阅读论文或者贡献开源或参加一些课程），这样才能有稳定、持续的进步，例如每周阅读2～3篇论文，一年就至少有了100篇论文的积累，而不是光想着花周末的时间去卷 👍👍👍</li>
</ol>
<h3> 2.3 如何选择好的工作</h3>
<ol>
<li>选择一个有很棒的人和项目的团队：因为我们都会深深受身边人的影响，他们能教你多少，他们的努力程度，他们的眼界格局、人脉资源等都会深刻地影响我们未来的发展</li>
<li><strong>选择一个好的小团队，特别是好的leader，而不是公司的品牌</strong>：工作时接触最多的是团队中的10～30个人，特别是直接带你的leader，这才是选职位时应该着重关注的，而不是公司品牌大小，大公司内部方差极大，所以宁愿去小公司的精英团队，也不要去大公司的垃圾团队 👍👍👍</li>
<li>如果在找工作时公司方拒绝告诉你做什么项目，leader是谁，那就该好好掂量掂量这份工作的含金量了</li>
<li>注意公司的轮换制度：千万要小心进入公司后等公司分配的工作，最好事先就确定好团队，如果公司提前保证了将来会让你轮换到你心仪的团队，那还是可以接受的</li>
<li>多学习，多做重要的工作！！！</li>
</ol>
<h3> 2.4 【学生提问】什么是有意义的工作</h3>
<ol>
<li>大公司的AI偏同质化，都在卷最赚钱的几个赛道，例如人脸识别，语音识别，搜广推等等</li>
<li>但其实AI可以和许多传统行业相结合，改造传统行业</li>
<li>挖掘冷门的新赛道也是一件激动人心，具有挑战性的事情，也许会从中发现更好的机会</li>
</ol>
<hr>
<h2> 3 【李沐】如何读论文</h2>
<p>视频连接：<a href="https://www.bilibili.com/video/BV1H44y1t75x/?spm_id_from=333.999.0.0&amp;vd_source=e8390493db96c0c1ca516a431544c5d8" target="_blank" rel="noopener noreferrer">link</a></p>
<h3> 3.1 论文结构</h3>
<ol>
<li>titile</li>
<li>abstract</li>
<li>introduction</li>
<li>method</li>
<li>experiments</li>
<li>conclusion</li>
</ol>
<h2> 3.2 三步法读文献</h2>
<ul>
<li>第一遍：标题、摘要、结论，看看是否和自己研究的相关。看完之后可以看看方法和实验部分重要的图和表。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。</li>
<li>第二遍：确定论文值得读之后，忽略部分细节，快速的把整个论文过一遍，需要了解重要的图和表，看懂文章针对的问题、解决方法、效果怎么样，知道每一个部分在干什么，圈出相关文献。觉得文章有看不懂的地方，可以读相关的引用文献。</li>
<li>第三遍：最详细的一遍，知道每一段和每一句干什么，脑补过程、换位思考、进行延伸</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>1.【经典CNN架构】AlexNet 论文精读</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/1.%20AlexNet.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/1.%20AlexNet.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">1.【经典CNN架构】AlexNet 论文精读</source>
      <description>1.【经典CNN架构】AlexNet 论文精读 原文链接：ImageNet Classification with Deep Convolutional Neural Networks 0. 核心总结</description>
      <pubDate>Wed, 03 May 2023 05:18:33 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 1.【经典CNN架构】AlexNet 论文精读</h1>
<blockquote>
<p>原文链接：<a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank" rel="noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks</a></p>
</blockquote>
<h2> 0. 核心总结</h2>
<p>核心是提出了一种深层卷积神经网络的架构，并引入了GPU训练、ReLU激活函数、Dropout正则化这些训练深层卷积神经网络的手段。</p>
<h2> 1. 摘要</h2>
<p>训练了一个大型的卷积神经网络，在ImageNet LSVRC-2010和ILSVRC-2012图像分类竞赛中取得了远低于第二名错误率的成绩。</p>
<h2> 2. 引言</h2>
<p>事实证明，大图像数据集比小图像数据集能够训练更优的模型，但同时对于ImageNet这样的超大型数据集也需要与之对应的具有强大学习能力的模型，CNN对图像的性质（局部平稳和参数共享）使其比相同性能的标准前馈神经网络参数量要少得多。</p>
<p>尽管CNN具有如此优秀的性质，但计算它还是很昂贵的，然而得益于GPU使得训练大型的CNN网络成为了可能，并且大型图像数据集ImageNet的出现使得模型不会出现严重的过拟合。</p>
<p>本文的具体贡献如下：</p>
<ol>
<li>在ImageNet子集上训练了迄今为止最大的卷积神经网络，并取得了迄今为止最好的成绩；</li>
<li>编写了一个高度优化的2D卷积的GPU实现，以及提供了训练卷积神经网络的所有操作细节，包括如何提高性能、减少训练时间等；</li>
<li>即使有大量的数据，但由于网络的太大还是会有过拟合的问题，所以采用了几种有效的技术来防止过拟合。</li>
</ol>
<h2> 3. 数据集</h2>
<p>使用了著名的ImageNet数据集，介绍直接从原文翻译。</p>
<blockquote>
<p>ImageNet是一个由超过1500万张标注的高分辨率图像组成的数据集，属于大约22，000个类别。这些图像是从网上收集的，并由人类贴标者使用亚马逊的研究工具众包工具进行标记。从2010年开始，作为Pascal视觉对象挑战赛的一部分，每年举办一次名为ImageNet大规模视觉识别挑战赛( ILSVRC )的竞赛。ILSVRC使用ImageNet的一个子集，在1000个类别中的每个类别中大约有1000个图像。总共有大约120万张训练图像、50,000张验证图像和150,000张测试图像。<br>
ILSVRC-2010是唯一一个测试集标签可用的ILSVRC版本，因此这是我们进行大部分实验的版本。由于我们也在ILSVRC-2012竞赛中输入了我们的模型，在第6节中我们也报告了我们在这个版本的数据集上的结果，对于这个版本的数据集，测试集标签是不可用的。在ImageNet上，通常报告两个错误率：top-1和top-5，其中top-5错误率是模型认为最可能的5个标签中没有正确标签的测试图像的分数。<br>
ImageNet由可变分辨率的图像组成，而我们的系统需要一个恒定的输入维度。因此，我们将图像降采样到256×256的固定分辨率。给定一个矩形图像，我们首先对图像进行缩放，使较短的边长为256，然后从结果图像中裁剪出中心的256×256块。我们没有以任何其他方式预处理图像，除了从每个像素中减去训练集的平均活动。因此，我们在像素的(中心)原始RGB值上训练我们的网络。</p>
</blockquote>
<h2> 4. 架构</h2>
<p>AlexNet的网络架构如下图所示，它包含5个卷积层和3个全连接层，下面会介绍AlexNet网络一些新颖的特征，以下特征根据重要性向下排序。</p>
<figure><img src="https://i.imgur.com/5F8HDJg.png" alt="AlexNet网络架构图" tabindex="0" loading="lazy"><figcaption>AlexNet网络架构图</figcaption></figure>
<h3> 4.1 ReLU激活函数</h3>
<p>ReLU激活函数的公式：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p>
<p>本文通过大量实验证明使用ReLU作为激活函数比使用tanh作激活函数训练速度要快好几倍，如下图所示：</p>
<figure><img src="https://i.imgur.com/uotVyNs.png" alt="relu与tanh速度对比图" tabindex="0" loading="lazy"><figcaption>relu与tanh速度对比图</figcaption></figure>
<h3> 4.2 多GPU训练</h3>
<p>本文使用两块GTX 580 GPU来训练AlexNet，采用并行化方案，每个GPU上放置一半的内核。</p>
<h3> 4.3 局部响应归一化（LRN，Local Response Normalization）</h3>
<p>一种提高泛化性的手段，现在已不常用，被批归一化Batch Normalization代替。</p>
<h3> 4.4 重叠池化（Overlapping Pooling）</h3>
<p>即步长要小于池化单元的大小，使得每一个池化单元组成的网格有所重叠，这样能稍微降低点错误率。</p>
<h3> 4.5 整体架构</h3>
<p>AlexNet包含8个带权重的层，前5个为卷积层，其余3个为全连接层，最后输出到一个1000类的Softmax层。</p>
<p>第2层，第4层，第5层卷积层内核仅连接到位于同一GPU上的前一层内核。第3层内核连接到第二层的所有内核，全连接层也与上一层的所有神经元相连。局部相应归一化应用于第1层和第2层，最大池化层是重叠池化，使用在第1层，第2层和第5层。</p>
<h2> 5. 减少过拟合</h2>
<p>第一种方式是使用数据增强，包括以下两种形式：</p>
<ol>
<li>通过图像平移和水平反射生成图像；</li>
<li>改变训练图像中RGB通道的强度，即对整个ImageNet训练集的RGB像素值集合进行PCA主成分分析，将权重加到对应的RGB通道上。</li>
</ol>
<p>第二种方式是使用Dropout方法，即在每轮前向传播和反向传播中，每层的神经元将会有一定的比例“失活”，即在本轮置为0，不参与前向传播和反向传播的过程。</p>
<h2> 6. 训练细节</h2>
<p>优化器为SGD，batch size为128，momentum为0.9，权重衰减（L2正则）为5e-4。</p>
<p>从一个均值为0，标准差为0.01的高斯分布中初始化各层权重，用常数1初始化第2层，第4层，第5层和全连接层的偏置，用常数0初始化其余层的偏置。</p>
<p>每层的学习率相同，并在训练过程中手动调整。学习率初始化为0.01，当验证集错误率停止下降时，将学习率减小10倍继续训练，在终止训练前减小3次。</p>
<p>本文在120万张图像训练集上训练了90个epoch，在两个NVDIA GTX 580 3GB GPU上训练了5～6天的时间。</p>
<h2> 7. 结果</h2>
<p>在ILSVRC-2010测试集上与其它方法的对比结果：</p>
<figure><img src="https://i.imgur.com/9ZvQf02.png" alt="AlexNet测试结果图1" tabindex="0" loading="lazy"><figcaption>AlexNet测试结果图1</figcaption></figure>
<p>在ILSVRC-2012验证集和测试集上的对比结果，*号表示在完整版ImageNet数据集上进行过预训练再微调，1 CNN表示一个标准的AlexNet，5 CNN表示训练5个AlexNet将预测结果取平均，7 CNN表示在AlexNet最后一个池化层后再加上第6个卷积层：</p>
<figure><img src="https://i.imgur.com/A8guPdw.png" alt="AlexNet测试结果图2" tabindex="0" loading="lazy"><figcaption>AlexNet测试结果图2</figcaption></figure>
<p>下图展示了第1层卷积层的96个卷积核的可视化情况，表征了卷积核所学习到的特征：</p>
<figure><img src="https://i.imgur.com/f3WtVFM.png" alt="AlexNet卷积核可视化" tabindex="0" loading="lazy"><figcaption>AlexNet卷积核可视化</figcaption></figure>
<p>下图展示了AlexNet在8张测试图像上的top-5预测结果，可以发现即使是偏离中心的对象（如第1幅图）也可以被网络识别，并且大多数图像top-5预测结果尽管并非完全正确但似乎也是合理的，同时也可以注意到部分图像（如第7幅图）本身就具有模糊性：</p>
<figure><img src="https://i.imgur.com/2OnlTqo.png" alt="AlexNet图像预测" tabindex="0" loading="lazy"><figcaption>AlexNet图像预测</figcaption></figure>
<p>下图展示了在AlexNet最后一个隐藏层的特征向量欧氏距离最小的几幅图像（每行），可以发现特征向量欧式距离相近的图片确实表征的是同一个对象，即使对象的位置、朝向、明暗不同，这表明卷积神经网络可以很好的提取出图像的语义特征</p>
<figure><img src="https://i.imgur.com/9ivEjgY.jpg" alt="AlexNet最后一个隐藏层所表征的特征向量" tabindex="0" loading="lazy"><figcaption>AlexNet最后一个隐藏层所表征的特征向量</figcaption></figure>
<h2> 8. 思考</h2>
<p>本文认为卷积神经网络的深度是很重要的，减小深度会导致性能的下降。</p>
<p>AlexNet虽然在图像分类任务上取得了很大的提升，但对比人类视觉系统的识别还是有一定差距的。</p>
]]></content:encoded>
      <enclosure url="https://i.imgur.com/5F8HDJg.png" type="image/png"/>
    </item>
    <item>
      <title>2.【经典CNN架构】ResNet 论文精读</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/2.%20ResNet.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/2.%20ResNet.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">2.【经典CNN架构】ResNet 论文精读</source>
      <description>2.【经典CNN架构】ResNet 论文精读 原文链接：Deep Residual Learning for Image Recognition 0. 核心总结</description>
      <pubDate>Wed, 03 May 2023 05:18:33 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 2.【经典CNN架构】ResNet 论文精读</h1>
<blockquote>
<p>原文链接：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener noreferrer">Deep Residual Learning for Image Recognition</a></p>
</blockquote>
<h2> 0. 核心总结</h2>
<p>核心是提出了一种<strong>残差结构</strong>，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。</p>
<h2> 1. 摘要</h2>
<p>更深的神经网络更难训练，本文提出了一个残差学习框架，以减轻深层神经网络训练的难度。本文将网络的层重构为引入层输入的可学习残差函数，代替了之前为引入层输入的函数。</p>
<p>本文提供了全面的实验证明这些残差网络更容易优化，并且能够从大幅增加的深度中获得准确率增益。在ImageNet数据集上，本文测试了152层的残差网络（比VGG网络深8倍，但具有更低的复杂度），在测试集上达到了3.57%的误差，这个结果赢得了ILSVRC 2015分类任务的第一名。本文也在CIFAR-10数据集上展示了100层和1000层残差网络的分析结果。</p>
<p>网络的深度对于许多视觉识别任务来说是非常重要的，仅仅由于本文实现了极深网络的训练，本文在COCO目标检测数据集上获得了28%的相对提升。深度残差网络也是作者在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务上取得第1名的基础。</p>
<h2> 2. 引言</h2>
<p>卷积神经网络的深度是非常重要的，之前在ImageNet数据集上取得领先结果的经典模型都从深度上受益，但是现在深度卷积神经网络正面临臭名昭著的梯度消失/爆炸问题。</p>
<p>虽然这个问题目前可以通过Normalization来解决，但却又出现了退化问题：随着网络深度的增加，精度达到饱和，然后迅速退化，即<strong>更深的网络精度反而下降</strong>。这种退化并不是由过拟合引起的，因为训练误差同样增高，实验结果如下图：</p>
<figure><img src="https://i.imgur.com/pnZMQpr.png" alt="不同层数卷积神经网络的训练和测试误差" tabindex="0" loading="lazy"><figcaption>不同层数卷积神经网络的训练和测试误差</figcaption></figure>
<p>根据朴素的直觉，即使更深的网络的深层部分“什么都不做”，即输出是输入的恒等映射，深层网络起码也不应该比浅层网络的误差高，但这样的实验结果表明目前的优化器找不到这样的解。</p>
<p>因此，本文引入了一个残差块来解决退化问题，如下图所示，残差块的核心是通过一个捷径连接（Shortcut Connections，或叫做残差连接）跳过一层或多层卷积将输出增加到层的输出中，添加一个输入的恒等映射。</p>
<p>用数学公式表达，即我们希望让卷积层拟合一个残差映射 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> ，而不是原映射 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> ，因为通过实验发现卷积层这样的非线性层很难拟合一个恒等映射，所以我们直接增加一个恒等映射。</p>
<figure><img src="https://i.imgur.com/VFWqkqe.png" alt="残差块结构示意图" tabindex="0" loading="lazy"><figcaption>残差块结构示意图</figcaption></figure>
<p>通过在ImageNet数据集上的全面实验表明：</p>
<ol>
<li>本文的深度残差网络更易于优化，相比简单堆叠的“普通网络”有更低的训练误差；</li>
<li>本文的深度残差网络可以很容易得从大幅增加的深度中获得精度增益，产出的结果明显优于以前的网络。</li>
</ol>
<p>本文也在CIFAR-10数据集上做了同样的测试，得到了同样的优异的精度表现，并探索训练了超过1000层的模型。</p>
<p>本文训练的152层残差网络在ImageNet测试集上取得了3.57%的top-5误差，同时仍比VGG网络有更低的复杂度，赢得了ILSVRC 2015分类竞赛的第一名；极深的网络在其它识别任务上也具有出色的泛化性能，在ILSVRC和COCO 2015竞赛中进一步获得第1名（ImageNet检测、ImageNet定位、COCO检测和COCO分割）。这表明残差学习的原理具有普适性，它在其他视觉和非视觉问题中也是同样适用的。</p>
<h2> 3. ResNet网络细节</h2>
<h3> 3.1 如何增加残差连接</h3>
<p>在数学公式上，残差块的定义如下：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">})</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span></p>
<p>其中， <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 分别为层的输入和输出向量，函数 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">})</span></span></span></span> 表示要学习的残差映射。</p>
<p>例如，对于上一幅图所示的两层残差块， <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em;">LU</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> ，为了简化表示此处省略了偏差（Bias）。接着，残差连接通过 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 操作逐元素相加来实现，相加后再通过一层ReLU输入下一个残差块。</p>
<p>此外需要注意，在等式中 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span></span></span> 的维度必须相等，若不相等则可以通过一个线性投影 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 来匹配维度，如下式：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">})</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span></span></span></span></span></p>
<p>此外，以上公式符号为了简单起见表示的是全连接层，但对于卷积层同样也是使用的，函数 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">})</span></span></span></span> 可以表示多个卷积层，逐元素相加在卷积层上即逐通道相加，线性映射在卷积层中一般通过一个1×1的卷积核来实现维度匹配。</p>
<h3> 3.2 网络架构</h3>
<p>下图展示了34层的ResNet架构，并与34层的“普通”网络及VGG-19的结构对比：</p>
<figure><img src="https://i.imgur.com/hhl8lxx.png" alt="ResNet-34架构与其它架构的对比" tabindex="0" loading="lazy"><figcaption>ResNet-34架构与其它架构的对比</figcaption></figure>
<p>下表给出了ResNet架构的更多细节及更多变体：</p>
<figure><img src="https://i.imgur.com/Oz8YCZD.png" alt="ResNet不同架构的配置" tabindex="0" loading="lazy"><figcaption>ResNet不同架构的配置</figcaption></figure>
<p>其中，对于50层以上的深层网络，为了降低其计算复杂度，本文设计了一种瓶颈架构（Bottleneck），如下图所示。即先通过一个1×1的卷积先降低维度，再通过具有较小输入和输出维度的3×3卷积瓶颈，最后再通过一个1×1的卷积恢复维度。通过这样的瓶颈设计可以有效地减少模型的参数两和复杂度。</p>
<figure><img src="https://i.imgur.com/iOc24tZ.png" alt="屏幕截图 2023-03-26 103814" tabindex="0" loading="lazy"><figcaption>屏幕截图 2023-03-26 103814</figcaption></figure>
<h3> 3.3 网络训练细节</h3>
<ol>
<li>数据增强：对图像的短边在[256,480]中随机采样进行尺度缩放；对图像或其随机翻转进行224×224的随机剪裁采样，并减去每个像素的均值；标准颜色增强。</li>
<li>在每次卷积之后、激活之前采用批归一化（Batch Normalization）。</li>
<li>初始化权重从头开始训练，使用SGD优化器，批大小（Batch Size）为256；学习率从0.1开始，当loss不再下降时除以10；最多训练 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">60</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span> 个迭代（iteration）；权重衰减（Weight Decay）为0.0001，动量（Momentum）为0.9；不使用Dropout。</li>
<li>（为了刷榜，没多大必要）在测试时采用标准的10-crop测试，为了获得最佳结果，采用全卷积网络形式，并在多尺度图像上取平均精度（对图像的短边进行缩放在{224,256,384,480,640}中取值）。</li>
</ol>
<h2> 4. 实验与结果</h2>
<p>本文在由1000个类组成的ImageNet-2012分类数据集上进行评估。模型在128万张训练集图像上进行训练，在5万张验证集图像上进行测试，得到top-1和top-5错误率。</p>
<p>下表对比了不同层数的ResNet和普通网络在ImageNet验证集上的Top-1误差：</p>
<figure><img src="https://i.imgur.com/LQorajE.png" alt="ResNet和普通网络的对比" tabindex="0" loading="lazy"><figcaption>ResNet和普通网络的对比</figcaption></figure>
<p>下图展示了它们在训练过程中的训练/验证误差：</p>
<figure><img src="https://i.imgur.com/OKkMSIU.png" alt="ResNet和普通网络的训练过程" tabindex="0" loading="lazy"><figcaption>ResNet和普通网络的训练过程</figcaption></figure>
<p>上述图表结果均表明，普通网络的精度随着深度的增加反而会退化，而ResNet则可以从增加的深度中获得精度增益，同时可以发现，ResNet的收敛速度会比普通网络更快。</p>
<p>本文还研究了三种恒等映射方式：</p>
<ol>
<li>A：使用补零的方式来实现维度匹配；</li>
<li>B：不同维度则使用线性投影来实现维度匹配，相同维度则直接相加；</li>
<li>C：不论维度是否匹配均使用线性投影。</li>
</ol>
<p>实验结果如下表所示，下表为10-crop测试结果，可以发现C略优于B，优于A，但使用C会导致参数量和复杂度过大，所以接下来的实验均采用B方案。</p>
<figure><img src="https://i.imgur.com/lfMBvg5.png" alt="不同网络架构的10-crop测试结果" tabindex="0" loading="lazy"><figcaption>不同网络架构的10-crop测试结果</figcaption></figure>
<p>此外，上表还展示了ResNet-50/101/152三种深层ResNet架构的测试结果，可以发现没有出现退化现象，均从深度中获得了精度增益。</p>
<p>下表展示了单模型的测试结果，得到的结论是相同的。</p>
<figure><img src="https://i.imgur.com/HBlhfBt.png" alt="不同网络架构的单模型测试结果" tabindex="0" loading="lazy"><figcaption>不同网络架构的单模型测试结果</figcaption></figure>
<p>此外，上述两个测试结果表均展示与之前SOTA单模型的对比结果，可以发现ResNet-50就由于之前所有的SOTA单模型。</p>
<p>本文还将6个不同深度的ResNet模型进行集成（两个152层，其余深度各一个），这个集成模型的精度最优，在ILSVRC 2015竞赛中取得了第一名，测试结果如下表所示，下表为与ImageNet测试集上排名前5的模型的对比结果：</p>
<figure><img src="https://i.imgur.com/RDubP6g.png" alt="与ImageNet测试集排名前5的模型的对比结果" tabindex="0" loading="lazy"><figcaption>与ImageNet测试集排名前5的模型的对比结果</figcaption></figure>
<h2> 5. 个人思考</h2>
<p>此前的卷积神经网络架构一直在追求更深的深度，因为能够从增加的深度中获得精度增益，但当深度到了某一界限，更深的层非但没有起作用，反而起了副作用，出现了梯度消失/爆炸的现象。</p>
<p>本文揭示了发生这种现象的本质原因是由于我们目前的优化器很难去优化到一个恒等映射，即输出对于输入总会发生一定程度的偏移，随着深度的增加，这种偏移会离最优解越偏越远，因此本文将前几层输入信息引入下一层，增加了下一层输入恒等映射的权重，让下一层有选择拟合恒等映射的空间。</p>
<p>个人的另一种理解是，神经网络的深层神经元会“遗忘”神经网络浅层的特征信息，所以需要通过不断加入浅层信息的方式去“提醒”深层的神经元这个输入的底层特征，来帮助深层神经元做出更优的推断。</p>
]]></content:encoded>
      <enclosure url="https://i.imgur.com/pnZMQpr.png" type="image/png"/>
    </item>
    <item>
      <title>3.【新神经网络架构】Transformer 论文精读</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/3.%20Transformer.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/AI%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/3.%20Transformer.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">3.【新神经网络架构】Transformer 论文精读</source>
      <description>3.【新神经网络架构】Transformer 论文精读 原文链接：Attention Is All You Need 源码链接：https://github.com/tensorflow/tensor2tensor</description>
      <pubDate>Wed, 03 May 2023 05:18:33 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 3.【新神经网络架构】Transformer 论文精读</h1>
<blockquote>
<p>原文链接：<a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener noreferrer">Attention Is All You Need</a>
源码链接：<a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener noreferrer">https://github.com/tensorflow/tensor2tensor</a></p>
</blockquote>
<h2> 0. 核心总结</h2>
<p>核心是提出了一种继CNN、RNN之后的新型神经网络架构Transformer，这种架构完全基于注意力机制，可以学习到更底层特征，在机器翻译上取得了SOTA的效果，并且目前对图像、音频、视频等领域也影响深远。</p>
<h2> 1. 摘要</h2>
<p>本文提出了一个全新的网络架构，称为Transformer，完全基于注意力机制，完全摒弃了卷积和循环神经网络。通过多个机器翻译实验表明，本文的Transformer模型在精度上更优，同时可以并行化计算和需要的训练时间更少。在WMT2014英德翻译任务上取得了28.4个BLEU，比包括集成模型在内的最好结果高了2个BLEU以上。</p>
<h2> 2. 引言</h2>
<p>RNN、LSTM、GRU是此前最为先进的序列模型，但是由于其顺序计算的特性，无法并行化。注意力机制（Attention）的提出，使得可以在不同位置之间建立长距离的依赖关系，从而可以并行化计算，通常这类注意力机制都与循环网络结合使用。</p>
<p>在本文的工作中，提出了一种Transformer结构，完全依靠注意力机制来绘制输入和输出之间的全局依赖关系。Transformer支持更多的并行化计算，在8个P100 GPU上训练短短12个小时后就可以达到翻译质量的先进水平。</p>
<h2> 3. Transformer模型结构</h2>
<p>Transformer模型的整体结构如下图所示，编码器和解码器都使用堆叠的<strong>自注意力（Self-attention）<strong>和</strong>逐点全连接层（Point-wise fully connected layers）</strong>，分别如图的左半部分和右半部分所示：</p>
<figure><img src="https://i.imgur.com/couWgCO.png" alt="Transformer模型结构" tabindex="0" loading="lazy"><figcaption>Transformer模型结构</figcaption></figure>
<p>大多数先进的序列模型都有编码器-解码器结构，在Transformer中，编码器将输入序列 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 映射为一个连续的表示 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord" style="text-shadow:0.02em 0.01em 0.04px;"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</p>
<p>给定 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord" style="text-shadow:0.02em 0.01em 0.04px;"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> ，解码器逐个生成输出序列 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ，一次一个元素，在每一次，模型都是自回归的，即在生成下一个元素 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 时，模型都是输入之前生成的元素 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 作为额外的输入。</p>
<h3> 3.1 编码器和解码器块细节</h3>
<ul>
<li>
<p><strong>编码器（Encoder）</strong>：编码器由 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">6</span></span></span></span> 个相同的层堆叠而成，每层包含两个子层。第一层是一个<strong>多头自注意力机制（Multi-head self-attention）</strong>，第二层是一个简单的<strong>逐点全连接前馈网络（Point-wise fully connected feed-forward network）</strong>。
每个子层都有一个<strong>残差连接（Residual Connection）</strong>，然后进行<strong>层归一化（Layer Normalization）</strong>，即每个子层的输出是 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">LayerNorm</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Sublayer</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span> 。为了便于进行残差连接，模型的所有子层和嵌入层输出的维度都是 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span> 。</p>
</li>
<li>
<p><strong>解码器（Decoder）</strong>：解码器同样也由 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">6</span></span></span></span> 个相同的层堆叠而成。解码器的结构基本与编码器相同，包含编码器中的两个子层，但解码器额外有的第三个子层：<strong>掩码多头注意力机制（Masked Multi-head Attention）</strong>。这个掩码是为了在训练和验证时不会看到未来的信息，即在生成第 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 个词时，只能看到前 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 个词，而不能看到第 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 个词及其后面的词。</p>
</li>
<li>
<p><strong>层归一化（Layer Normalization）</strong>：层归一化和批归一化（Batch Normalization）类似，但是批归一化是在一个batch中对每个样本的同一个特征进行归一化，而层归一化是对每个样本的不同特征进行归一化，如下图所示：</p>
</li>
</ul>
<figure><img src="https://i.imgur.com/VHXz0Mr.png" alt="层归一化示意图" tabindex="0" loading="lazy"><figcaption>层归一化示意图</figcaption></figure>
<h3> 3.2 注意力机制细节</h3>
<p>一个注意力函数可以将一个 <strong>查询（Query）</strong> 和一组 <strong>键（Key）值（Value）</strong> 对映射到一个输出，输出为值的加权和，权重由查询与对应的键的相似度决定。</p>
<h4> 3.2.1 缩放点积注意力（Scaled Dot-Product Attention）</h4>
<p>本文中的注意力函数是<strong>缩放点积注意力（Scaled Dot-Product Attention）</strong>，如下图所示：</p>
<figure><img src="https://i.imgur.com/G9U2T9s.png" alt="缩放点注意力计算示意图" tabindex="0" loading="lazy"><figcaption>缩放点注意力计算示意图</figcaption></figure>
<p>其中，查询和键的维度为 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，值的维度为 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。查询和键的矩阵乘法得到的矩阵除以 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1828em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span></span> ，然后进行softmax操作得到权重矩阵，最后与值的矩阵乘法得到输出矩阵，计算公式如下：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<h4> 3.2.2 多头注意力（Multi-head Attention）</h4>
<p>与单独使用注意力函数相比，使用多头注意力函数将查询、键和值线性投影到 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> 个不同的 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 、<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 个维度是有益的。我们还可以并行计算这 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> 个头，生成 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 维度的输出值，然后将这些值拼接起来再次进行线性投影，得到最终的输出值，如下图所示：</p>
<figure><img src="https://i.imgur.com/oiI4qnU.png" alt="多头注意力示意图" tabindex="0" loading="lazy"><figcaption>多头注意力示意图</figcaption></figure>
<p>多头注意力允许模型在不同位置共同关注来自不同表示子空间的信息，在单个注意力头中，模型可能会忽略来自其他子空间的相关信息。</p>
<p>多头注意力机制的计算公式如下：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.1706em;vertical-align:-1.3353em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8353em;"><span style="top:-3.944em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span></span></span><span style="top:-2.3247em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord text"><span class="mord">where&nbsp;head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3353em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8353em;"><span style="top:-3.944em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord text"><span class="mord">Concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span><span style="top:-2.3247em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3353em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，投影为参数矩阵，它们的维度为 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> ，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> ，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> ，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8804em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 。</p>
<p>在本文实验中，采用 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">8</span></span></span></span> 个头，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span></span></span></span> 。由于每个头的维度减小，总的计算成本与单头注意力机制相似。</p>
<h3> 3.3 逐点前馈网络细节</h3>
<p>逐点前馈网络分别且相同地应用于输入地每个位置，这包含两个线性变换，中间有一个 ReLU 激活函数，计算公式如下：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">FFN</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，输入输出的维度数均为 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span> ，中间层的维度数为 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">ff</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2048</span></span></span></span> 。</p>
<h3> 3.4 位置编码细节</h3>
<p>不同于 RNN 和 CNN，Transformer 模型中没有显式的循环和卷积，因此模型需要一种方法来利用序列中单词的顺序信息。为了在序列中的单词中注入一些位置信息，本文采用了位置编码（Positional Encoding）。</p>
<p>位置编码与词嵌入具有相同的维度 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，因此最终输出为两者相加。位置编码的值是根据位置的正弦和余弦函数计算得到的，如下式：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.196em;vertical-align:-1.348em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.848em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.312em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.348em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.848em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-2.312em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.348em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span> 是位置，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 是维度。也就是说，位置编码的每个维度对应一个正弦曲线，波长逐渐增加。本文还尝试了其他编码方式，但是发现这种方式的效果最好。</p>
<h2> 4. 为什么使用自注意力</h2>
<p>本章将自注意力机制与 RNN 和 CNN 在三个指标上进行了比较。一是每层的计算复杂度；二是可并行的计算量，用所需的最小顺序操作数来衡量；三是最长路径内的依赖关系，用最大路径长度来衡量。</p>
<p>对比结果如下表所示，其中 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> 为序列长度，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span> 为每个位置的表示维度，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 为卷积核的大小，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span> 为受限自注意力中的领域大小：</p>
<figure><img src="https://i.imgur.com/Qg0zec2.png" alt="不同层类型的对比" tabindex="0" loading="lazy"><figcaption>不同层类型的对比</figcaption></figure>
<h2> 5. 训练细节</h2>
<h3> 5.1 训练数据集和批大小</h3>
<p>本文使用了标准的 WMT 2014 英语到德语的数据集，共包含 4.5M 句子对。句子被编码成约有 37000 tokens 的共享源-目标词汇表。</p>
<p>对于英语到法语，本文使用了更大的 WMT 2014 英语到法语的数据集，共包含 36M 句子对，拆分成 32000 tokens 词块词汇表。具有近似序列长度的数据被分配到同一批中，每个批包含约 25000 个源 tokens 和 25000 个目标 tokens。</p>
<h3> 5.2 训练硬件和训练计划</h3>
<p>本文使用了 8 块 NVIDIA P100 GPU 进行训练。对于基础模型，每步大约需要0.4秒，共训练了10万步（12小时）。对于大模型，每步时长约为1秒，共训练了30万步（3.5天）。</p>
<h3> 5.3 优化器</h3>
<p>本文使用 Adam 优化器，其中 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.9</span></span></span></span>，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.98</span></span></span></span>，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">9</span></span></span></span></span></span></span></span></span></span></span></span>。对于学习率，本文按以下公式进行变化：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1555em;vertical-align:-0.2914em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.4086em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0.5</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2914em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1741em;vertical-align:-0.31em;"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0.5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1741em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1.5</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9251em;vertical-align:-0.31em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span></span></span></span> 为当前训练步数，<span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9251em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4000</span></span></span></span>。</p>
<h3> 5.4 正则化</h3>
<p>在训练过程中，本文使用三类正则化方法：</p>
<ul>
<li>
<p><strong>残差Dropout</strong>：本文将Dropout引用到每个子层的输出，然后再进行残差连接和层归一化。Dropout的概率为 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.1</span></span></span></span>。</p>
</li>
<li>
<p><strong>标签平滑（Label Smoothing）</strong>：本文在训练过程中使用了 <span class="katex"><span class="katex-mathml"></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.1</span></span></span></span> 的标签平滑。标签平滑的作用是让模型对于训练数据中的噪声更加鲁棒。</p>
</li>
</ul>
<h2> 6. 实验结果</h2>
<h3> 6.1 机器翻译结果</h3>
<p>下表总结了本文提出的Transformer架构在 WMT 2014 英语到德语和英语到法语数据集上与其它模型架构的结果：</p>
<figure><img src="https://i.imgur.com/LxUGkbg.png" alt="Transformer与其它模型架构的对比结果" tabindex="0" loading="lazy"><figcaption>Transformer与其它模型架构的对比结果</figcaption></figure>
<h3> 6.2 模型参数分析</h3>
<p>为了评估Transformer的不同参数的重要性，本文在 newstest2013 英德翻译数据集上进行了一系列实验。下表总结了实验结果：</p>
<figure><img src="https://i.imgur.com/eaqw6N1.png" alt="Transformer参数分析的结果" tabindex="0" loading="lazy"><figcaption>Transformer参数分析的结果</figcaption></figure>
<ol>
<li>在（A）中，改变了注意力头的数量和键值维度，结果表明，适量地增加头的数量可以提高性能，但太多的头也会降低性能。</li>
<li>在（B）中，可以发现降低键的维度会降低性能。</li>
<li>在（C）中，可以发现增加模型深度，即更大的模型可以提高性能。</li>
<li>在（D）中，可以发现使用正则化方法可以提高性能。</li>
<li>在（E）中，将正弦位置编码替换为可学习的位置嵌入，可以发现实际性能与基模型几乎相同。</li>
</ol>
<h2> 7. 个人思考</h2>
<p>本文的最大贡献就是提出了一种全新的神经网络架构Transformer，该架构影响深远，目前也已经广泛应用于图像、音频和视频等领域。</p>
<p>Transformer的核心注意力机制，顾名思义类似于人的注意力，查询、键和值分别可以认为是人的注意力上的自主性提示、非自主性提示和感官输入值。打个比方就是，当你在看一幅画时，你的主观意识会自主地注意画中的某个部分，比如一只猫，这就是查询；而当你看到画中的一只猫时，你的注意力会被非自主地吸引到猫的周围，比如猫的眼睛、耳朵、鼻子等，这就是键；最后，你会将这些感官输入值进行整合，形成对这幅画的理解，这就是值。</p>
<p>注意力机制乍一看和多层感知机MLP很像，无非就是权重和输入的加权和，但是注意力机制的权重是由输入决定的，而MLP的权重是固定的。这也是为什么注意力机制可以用于序列建模的原因，因为序列中的每个位置的输入都是不同的，所以注意力机制可以根据不同的输入来决定权重。</p>
<p>Transformer还有一个很大的特点是，Transformer具有很大的网络容量，需要学习很大的数据量才能学习到底层特征，更适合大规模数据集。这是由于Transformer结构缺少一些而类似CNN先天的归纳偏置，即卷积结构自带的先验知识，例如平移不变性和包含局部关系，因此在规模不足的数据集上表现没有那么好，需要更多的数据来学习到某种特征。</p>
]]></content:encoded>
      <enclosure url="https://i.imgur.com/couWgCO.png" type="image/png"/>
    </item>
    <item>
      <title>1.【经典综述】模型压缩和硬件加速综述 论文精读</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/%E9%AB%98%E6%95%88AI/1.%20%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E5%92%8C%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E7%BB%BC%E8%BF%B0.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%96%87%E7%8C%AE%E7%B2%BE%E8%AF%BB/%E9%AB%98%E6%95%88AI/1.%20%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E5%92%8C%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E7%BB%BC%E8%BF%B0.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">1.【经典综述】模型压缩和硬件加速综述 论文精读</source>
      <description>1.【经典综述】模型压缩和硬件加速综述 论文精读 原文链接：Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey 0. 核心总结 核心是提出了一种残差结构，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。</description>
      <pubDate>Wed, 03 May 2023 05:18:33 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 1.【经典综述】模型压缩和硬件加速综述 论文精读</h1>
<blockquote>
<p>原文链接：<a href="https://ieeexplore.ieee.org/abstract/document/9043731" target="_blank" rel="noopener noreferrer">Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey</a></p>
</blockquote>
<h2> 0. 核心总结</h2>
<p>核心是提出了一种<strong>残差结构</strong>，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。</p>
<h2> 0. 摘要</h2>
<p>本文回顾了几种主流的压缩方法，如<strong>紧凑模型</strong>（Compact Model）、<strong>张量分解</strong>（Tensor Decomposition）、<strong>数据量化</strong>（Data Quantization）和<strong>网络稀疏化</strong>（Network Sparsification），并对它们的压缩原理、评价指标、敏感性分析和联合使用进行了说明。</p>
<p>然后，本文回答了如何在神经网络加速器设计中利用这些方法的问题，并展示了最先进的硬件架构。</p>
<p>最后，本文讨论了公平比较、测试工作量、自动压缩、对安全性的影响和对框架/硬件层面的支持性等几个现存的问题，并阐述了该领域的研究热点和可能面临的挑战。</p>
<p>本文的目的是使读者快速构建起神经网络压缩和加速的大图景，清晰地评价各种方法，并为初步的研究提供指引。</p>
<h2> 1. 引言</h2>
<p>虽然现在具备高并行处理能力和高内存带宽的GPU很强大，但是它们的功耗和面积却很大，因此在移动设备上使用它们是不现实的，所以人们对能将DNN部署到对资源和能源受限的边缘设备的高效AI的兴趣越来越大。</p>
<p>算法与硬件的交互是神经网络压缩领域的趋势，如下图所示：</p>
<figure><img src="https://i.imgur.com/MVq5NCC.png" alt="DNN压缩和加速概览图" tabindex="0" loading="lazy"><figcaption>DNN压缩和加速概览图</figcaption></figure>
<p>在算法侧，近些年各式各样的算法被提出，但可以归类为以下4类:</p>
<ol>
<li>紧凑模型（Compact Model）：通过设计更紧凑的网络结构来减少参数量和计算量，如MobileNet、ShuffleNet、SqueezeNet等。</li>
<li>张量分解（Tensor Decomposition）：通过张量分解来减少参数量和计算量，如CP分解、Tucker分解、TT分解等。</li>
<li>数据量化（Data Quantization）：通过量化权重和激活值来减少参数量和计算量，如INT8、INT4、INT2、二值网络等。</li>
<li>网络稀疏化（Network Sparsification）：通过稀疏化权重和激活值来减少参数量和计算量，如剪枝、稀疏矩阵、稀疏矩阵分解等。</li>
</ol>
<p>在硬件侧，许多新型的DNN加速器被提出，如TPU、EIE、Eyeriss、DaDianNao等，在这些新型的DNN加速器的设计中，区别于传统加速器只优化硬件结构，它们以一定的精度损失为代价，考虑了多种压缩技术。高层次的算法优化为设计更高效的硬件提供指导，低层次的硬件设计为设计更有效的算法提供反馈，这就是<strong>算法-硬件协同设计</strong>。这种协同设计在现代DNN加速器的设计中无处不在，以提高性能。</p>
<p>本文的所有内容总结于下表，我们的解读文章也按表中所列的章节序号进行章节编排。</p>
<figure><img src="https://i.imgur.com/Nw36Rnw.png" alt="文章内容导引" tabindex="0" loading="lazy"><figcaption>文章内容导引</figcaption></figure>
<h2> 2. 神经网络的简要预备知识</h2>
]]></content:encoded>
      <enclosure url="https://i.imgur.com/MVq5NCC.png" type="image/png"/>
    </item>
    <item>
      <title>lesson7. 保存和加载模型</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson7_saveload.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson7_saveload.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson7. 保存和加载模型</source>
      <description>lesson7. 保存和加载模型 import torch import torchvision.models as models</description>
      <pubDate>Wed, 26 Apr 2023 12:18:25 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson7. 保存和加载模型</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2> 保存和加载模型权重</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 保存和加载整个模型结构和权重</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2> 保存和加载训练时的断点</h2>
<h3> 1.导入必要的库</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3> 2.定义和初始化神经网络</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h3> 3.初始化优化器</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3> 4.保存一般断点</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3> 5.加载一般断点</h3>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h3> 6.迁移学习下的热启动模式</h3>
<ul>
<li>只需设置strict=False来忽略非匹配的模型层参数</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson0. GPU性能测试</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/gpu_test.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/gpu_test.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson0. GPU性能测试</source>
      <description>lesson0. GPU性能测试 import torch import time print(torch.__version__) # 返回pytorch的版本 print(torch.cuda.is_available()) # 当CUDA可用时返回True a = torch.randn(10000, 1000) # 返回10000行1000列的张量矩阵 b = torch.randn(1000, 2000) # 返回1000行2000列的张量矩阵 t0 = time.time() # 记录时间 c = torch.matmul(a, b) # 矩阵乘法运算 t1 = time.time() # 记录时间 print(a.device, t1 - t0, c.norm(2)) # c.norm(2)表示矩阵c的二范数 device = torch.device(&amp;apos;cuda&amp;apos;) # 用GPU来运行 a = a.to(device) b = b.to(device) # 初次调用GPU，需要数据传送，因此比较慢 t0 = time.time() c = torch.matmul(a, b) t2 = time.time() print(a.device, t2 - t0, c.norm(2)) # 这才是GPU处理数据的真实运行时间，当数据量越大，GPU的优势越明显 t0 = time.time() c = torch.matmul(a, b) t2 = time.time() print(a.device, t2 - t0, c.norm(2))</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson0. GPU性能测试</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson1. 张量操作</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson1_tensor.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson1_tensor.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson1. 张量操作</source>
      <description>lesson1. 张量操作 import torch import numpy as np</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson1. 张量操作</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2> 初始化张量</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 张量的属性</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 张量运算</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 与Numpy互相转换</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
]]></content:encoded>
    </item>
    <item>
      <title>lesson2. 数据集和数据加载</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson2_data.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson2_data.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson2. 数据集和数据加载</source>
      <description>lesson2. 数据集和数据加载 import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson2. 数据集和数据加载</h1>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<h2> 加载预加载数据集</h2>
<ul>
<li>root ：训练集/测试集存储路径</li>
<li>train ：确定是训练集还是测试集</li>
<li>download=True ：如果root不可用 从互联网上下载数据</li>
<li>transform / target_transform ：指定特征和标签的转换</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 迭代和可视化数据集</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://i.imgur.com/Ui7pdoU.png" alt="result" tabindex="0" loading="lazy"><figcaption>result</figcaption></figure>
<h2> 创建自定义数据集</h2>
<ul>
<li>__init__ ：创建类对象时运行一次 包括图像 标注 和 变换</li>
<li>__len__ ：返回数据集样本数</li>
<li>__getitem__ ：根据索引加载和返回数据样本 包括图像 标注 和 变换</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 使用DataLoaders训练Data</h2>
<ul>
<li>DataLoader的作用：minibatches采样，每epoch reshuffle data，多线程加速</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> 通过DataLoader迭代</h2>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>
<figure><img src="https://i.imgur.com/UEoa2W0.png" alt="result" tabindex="0" loading="lazy"><figcaption>result</figcaption></figure>

]]></content:encoded>
      <enclosure url="https://i.imgur.com/Ui7pdoU.png" type="image/png"/>
    </item>
    <item>
      <title>lesson3. 数据预处理</title>
      <link>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson3_transform.html</link>
      <guid>https://lisenjie757.github.io/%E7%9F%A5%E8%AF%86%E5%BA%93/PyTorch%E5%85%A5%E9%97%A8/lesson3_transform.html</guid>
      <source url="https://lisenjie757.github.io/rss.xml">lesson3. 数据预处理</source>
      <description>lesson3. 数据预处理 图像和标签的转换函数 ToTensor() ：转换PIL图像和NumPy数组为FloatTensor，并归一化图像像素值为[0.,1.] Lamda Transforms ：此函数为用户自定义lamda函数；此处函数为将整数转换为one-hot编码，首先创建zero张量，长度取决于类别数，然后调用scatter_函数通过整数y对张量进行1填充 scatter_函数填充原理(dim=0) ：self[ index[i][j] ] [j] = src[i][j]</description>
      <pubDate>Wed, 26 Apr 2023 11:51:50 GMT</pubDate>
      <content:encoded><![CDATA[<h1> lesson3. 数据预处理</h1>
<h2> 图像和标签的转换函数</h2>
<ul>
<li>ToTensor() ：转换PIL图像和NumPy数组为FloatTensor，并归一化图像像素值为[0.,1.]</li>
<li>Lamda Transforms ：此函数为用户自定义lamda函数；此处函数为将整数转换为one-hot编码，首先创建zero张量，长度取决于类别数，然后调用scatter_函数通过整数y对张量进行1填充</li>
<li>scatter_函数填充原理(dim=0) ：self[ index[i][j] ] [j] = src[i][j]</li>
</ul>
<div class="language-python line-numbers-mode" data-ext="py"><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>]]></content:encoded>
    </item>
  </channel>
</rss>