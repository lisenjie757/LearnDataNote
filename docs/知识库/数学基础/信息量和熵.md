# 信息量和熵

## 1. 信息量

任何事件都是概率事件，是概率事件则承载着信息量，概率越小的事件它承载的信息量就越大，因为它越不可能发生，反之如果是既定事实，那么它的信息量就为0。
例如，狗咬人不算信息，人咬狗才算信息嘛。
所以若事件$x$发生的概率为$p(x)$，那么它的信息量为：$I(x)=-\log(p(x))$

## 2. 熵

熵就是信息量的期望，代表这个事件所有的可能性。
假设事件$X$共有n种可能性，发生$x_i$的概率为$p(x_i)$，那么该事件的熵为：
$$H(X) = -\sum^n_{i=1} p(x_i)\log(p(x_i))$$

## 3. KL散度（相对熵）

假设随机变量$X$有两个单独的概率分布函数$p(x)$和$q(x)$，我们可以使用KL散度来衡量这两个分布的差异。
在机器学习中，$p(x)$往往用来表示样本的真实分布，$q(x)$用来表示模型所预测的分布，那么KL散度就可以计算两个分布的差异，即Loss损失函数，计算公式如下：
$$KL(p \parallel q) = \sum^n_{i=1} p(x_i) \log(\frac{p(x_i)}{q(x_i)})$$
KL散度具有以下性质：
**性质1.** KL散度是不对称的。
**性质2.** KL散度是非负的，即$KL(p \parallel q) \geq 0$。且当$p=q$时，$KL(p \parallel q) = 0$。

- 证明：对KL散度计算公式进行以下变换：

$$KL(p \parallel q) = \sum^n_{i=1} p(x_i) \log(\frac{p(x_i)}{q(x_i)})= \mathbb{E}(\log \frac{p(X)}{q(X)}) = \mathbb{E}(-\log \frac{q(X)}{p(X)})$$
由于对数函数是凹函数，根据 [Jensen不等式](https://en.wikipedia.org/wiki/Jensen%27s_inequality)，可得：
$$\mathbb{E}(-\log \frac{q(X)}{p(X)}) \geq -\log\mathbb{E}(\frac{q(X)}{p(X)}) = -\log \sum^n_{i=1} p(x_i)\frac{q(x_i)}{p(x_i)} \\= -\log \sum^n_{i=1} q(x_i) = -log 1 = 0$$

## 4. 交叉熵

将KL散度公式变形：
$$\begin{aligned} 
KL(p \parallel q) &= \sum^n_{i=1} p(x_i) \log(\frac{p(x_i)}{q(x_i)}) \\
&= \sum^n_{i=1} p(x_i) \log(p(x_i)) - \sum^n_{i=1} p(x_i) \log(q(x_i)) \\
&= -H(p(X)) + [- \sum^n_{i=1} p(x_i) \log(q(x_i))]
\end{aligned}$$
可见等式的前一部分是分布$p$的熵，等式的后一部分就是交叉熵。
在机器学习中，需要构造Loss损失函数去评估真实值和预测值之间的差距，那么使用KL则刚刚好，即$KL(y \parallel \tilde{y})$。由于在大部分有监督学习问题中，真实值$y$是不变的，即KL散度的前半部分$y$的熵$H(y)$是不变的，故在优化过程中只需要关注交叉熵就好了，所以一般就直接使用交叉熵函数作为Loss损失函数。

## 5. JS散度

JS散度也是度量了两个概率分布的相似度，是KL散度的变体，解决了KL散度非对称的问题，其定义如下：
$$JS(p \parallel q) = \frac 12 KL(p \parallel \frac{p+q}2) + \frac 12 KL(p \parallel \frac{p+q}2)$$
JS散度具有以下性质：
**性质1.** JS散度是对称的。
**性质2.** JS散度是非负的，散度是非负的，即$JS(p \parallel q) \geq 0$。且当$p=q$时，$JS(p \parallel q) = 0$。

- 证明：由于KL散度大于等于0，易证得JS散度大于等于0。
